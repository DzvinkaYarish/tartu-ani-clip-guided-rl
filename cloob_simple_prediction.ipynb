{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "import cloob.clip as clip\n",
    "import cloob.zeroshot_data as zeroshot_data\n",
    "from cloob.clip import _transform\n",
    "from cloob.model import CLIPGeneral"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is  cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./cloob/model_configs/RN50.json\n"
     ]
    },
    {
     "data": {
      "text/plain": "CLIPGeneral(\n  (visual): ModifiedResNet(\n    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    (relu): ReLU(inplace=True)\n    (layer1): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (downsample): Sequential(\n          (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (avgpool): Identity()\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n      )\n    )\n    (attnpool): AttentionPool2d(\n      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n      (c_proj): Linear(in_features=2048, out_features=1024, bias=True)\n    )\n  )\n  (transformer): TextTransformer(\n    (transformer): Transformer(\n      (resblocks): Sequential(\n        (0): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (token_embedding): Embedding(49408, 512)\n    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CLOOB\n",
    "checkpoint_path = './checkpoints/cloob_rn50_yfcc_epoch_28.pt'\n",
    "configs_path = './cloob/model_configs/'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model_config_file = os.path.join(configs_path, checkpoint['model_config_file'])\n",
    "\n",
    "print('Loading model from', model_config_file)\n",
    "assert os.path.exists(model_config_file)\n",
    "with open(model_config_file, 'r') as f:\n",
    "    model_info = json.load(f)\n",
    "model = CLIPGeneral(**model_info)\n",
    "preprocess = _transform(model.visual.input_resolution, is_train=False)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    model.float()\n",
    "else:\n",
    "    model.to(device)\n",
    "\n",
    "sd = checkpoint[\"state_dict\"]\n",
    "sd = {k[len('module.'):]: v for k, v in sd.items()}\n",
    "if 'logit_scale_hopfield' in sd:\n",
    "    sd.pop('logit_scale_hopfield', None)\n",
    "model.load_state_dict(sd)\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "5.4933786"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_paths = [\n",
    "    \"./data/gtsrb/test/025/10553.png\",\n",
    "    \"./data/gtsrb/test/009/08086.png\",\n",
    "    \"./data/gtsrb/test/014/00621.png\",\n",
    "    \"./data/gtsrb/test/001/11095.png\",\n",
    "    \"./data/gtsrb/test/022/03702.png\",\n",
    "    \"./data/gtsrb/test/018/10780.png\",\n",
    "    \"./data/gtsrb/test/013/05310.png\",\n",
    "    \"./data/gtsrb/test/004/02686.png\",\n",
    "]\n",
    "classnames = zeroshot_data.gtsrb_classnames\n",
    "prompt_templates = zeroshot_data.gtsrb_templates\n",
    "\n",
    "image_path = image_paths[2]\n",
    "classname = classnames[14]\n",
    "\n",
    "# Image processing\n",
    "with torch.no_grad(), Image.open(image_path) as im:\n",
    "    image = preprocess(im).to(device)\n",
    "    image_embedding = model.encode_image(image.unsqueeze(0))\n",
    "    image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Text processing\n",
    "with torch.no_grad():\n",
    "    texts = [template(classname) for template in prompt_templates]\n",
    "    texts = clip.tokenize(texts).to(device)\n",
    "    class_embeddings = model.encode_text(texts)\n",
    "    class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "    class_embedding = class_embeddings.mean(dim=0)\n",
    "    class_embedding /= class_embedding.norm()\n",
    "\n",
    "similarity = (image_embedding @ class_embedding) * 30  # we scale by 30 just so the difference is more stark\n",
    "similarity.numpy()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}