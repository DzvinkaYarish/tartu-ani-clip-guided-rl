{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3fa1a94-14ff-4b0a-a918-d4e5f69b28c5",
   "metadata": {},
   "source": [
    "```bash\n",
    "conda create --name ani python=3.8\n",
    "conda install -c conda-forge jupyterlab\n",
    "pip install open_clip_torch\n",
    "pip install stable-baselines3[extra]\n",
    "pip install gym[all]\n",
    "pip install pyglet==1.5.27\n",
    "pip install tensorboardX\n",
    "conda install -c anaconda ipywidgets\n",
    "conda install -c anaconda scipy \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be7f60-1cd4-4afb-8829-342980047596",
   "metadata": {},
   "source": [
    "## Experiments\n",
    " - 2 environments: CartPole and LunarLander\n",
    " - clip models: openclip https://github.com/mlfoundations/open_clip(different models), cloob https://github.com/ml-jku/cloob\n",
    " - different prompts\n",
    "     - try several prompts at the same time which describe different states and then the rwd is formulated based on to which prompt the env image is most similar\n",
    "     - ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ee8f09a-40c5-4e52-bb46-75dc38d96a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import DQN, PPO\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ec5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_view_window():\n",
    "        from gym.envs.classic_control import rendering\n",
    "        org_constructor = rendering.Viewer.__init__\n",
    "\n",
    "        def constructor(self, *args, **kwargs):\n",
    "            org_constructor(self, *args, **kwargs)\n",
    "            self.window.set_visible(visible=False)\n",
    "\n",
    "        rendering.Viewer.__init__ = constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741daef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_view_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03579506-1dc0-4fbe-b6d8-b9bc6ad250fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEnv():\n",
    "    def __init__(self, env, clip_model, clip_preprocess, tokenizer, prompt, writer):\n",
    "        self.env = env\n",
    "        self.model = clip_model\n",
    "        self.preprocess = clip_preprocess\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_features = self.model.encode_text(self.tokenizer([prompt]))\n",
    "        self.text_features /= self.text_features.norm(dim=-1, keepdim=True)\n",
    "        if torch.cuda.is_available():\n",
    "            self.text_features = self.text_features.cuda()\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.metadata = self.env.metadata\n",
    "        self.clip_rewards_per_episode = []\n",
    "        self.env_rewards_per_episode = []\n",
    "        \n",
    "        self.clip_rewards = []\n",
    "        self.env_rewards = []\n",
    "        \n",
    "        self.writer = writer\n",
    "        self.n_steps = 0\n",
    "        self.n_episodes = 0\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def close(self):\n",
    "        return self.env.close()\n",
    "    \n",
    "    def step(self, action):\n",
    "        next_st, rwd, done, info = self.env.step(action)\n",
    "        img = self.env.render(mode=\"rgb_array\")\n",
    "        clip_rwd = self.get_clip_reward(img)\n",
    "        \n",
    "        if len(self.clip_rewards_per_episode) == 0:\n",
    "            self.clip_rewards_per_episode.append(clip_rwd)\n",
    "            self.env_rewards_per_episode.append(rwd)\n",
    "        else:\n",
    "            self.clip_rewards_per_episode.append(self.clip_rewards_per_episode[-1] + clip_rwd)\n",
    "            self.env_rewards_per_episode.append(self.env_rewards_per_episode[-1] + rwd)\n",
    "        \n",
    "     \n",
    "        if done:\n",
    "            self.writer.add_scalar('episode_length',  len(self.env_rewards_per_episode), self.n_episodes)\n",
    "            \n",
    "            #print(self.env_rewards_per_episode, self.clip_rewards_per_episode)\n",
    "            \n",
    "            self.writer.add_scalar('episode_rewards/env_reward',  sum(self.env_rewards_per_episode), self.n_episodes)\n",
    "            self.writer.add_scalar('episode_rewards/clip_reward', sum(self.clip_rewards_per_episode) , self.n_episodes)\n",
    "            \n",
    "            self.env_rewards.append(self.env_rewards_per_episode)\n",
    "            self.clip_rewards.append(self.clip_rewards_per_episode)\n",
    "\n",
    "            self.env_rewards_per_episode = []\n",
    "            self.clip_rewards_per_episode = []\n",
    "            \n",
    "            self.n_episodes += 1\n",
    "            \n",
    "        \n",
    "        self.n_steps += 1\n",
    "\n",
    "        return next_st, rwd, done, info\n",
    "    \n",
    "    def get_clip_reward(self, state):\n",
    "        image = self.preprocess(Image.fromarray(np.uint8(state))).unsqueeze(0)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = self.model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            if torch.cuda.is_available():\n",
    "                image_features = image_features.cuda()\n",
    "            sim = (image_features @ self.text_features.T)\n",
    "        return sim.cpu().detach().numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c9f0393-6e67-4f26-a621-603a1a731dd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file experiments already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb0acfd6-3d91-48cf-856a-ce30ea2eb63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(agent, env, prompt, clip_model_name, env_name, exp_path, n_steps, notes=''):\n",
    "    if not os.path.exists(exp_path):\n",
    "        os.mkdir(exp_path)\n",
    "    \n",
    "    agent.learn(total_timesteps=n_steps, progress_bar=True)\n",
    "    agent.save(f\"{exp_path}/agent\")\n",
    "    \n",
    "    corr = stats.pearsonr([sum(e) for e in env.env_rewards], [sum(e) for e in env.clip_rewards])[0]\n",
    "    m_rwd = np.mean([sum(e) for e in env.env_rewards[-10:]])\n",
    "    results = {\n",
    "        'env_name': env_name,\n",
    "        'prompt': prompt,\n",
    "        'clip_model_name': clip_model_name,\n",
    "        'correlation': corr,\n",
    "        'mean_env_rwd_over_last_10_episodes': m_rwd,\n",
    "        'n_episodes': env.n_episodes,\n",
    "        'n_steps': env.n_steps,\n",
    "         'notes': notes,\n",
    "    }\n",
    "    with open(f'{exp_path}/results.json', 'w') as f:\n",
    "        json.dump(results, f)\n",
    "    \n",
    "    # compute correlation between env and clip rewards for each episode separately\n",
    "    #print(env.env_rewards, env.clip_rewards)\n",
    "    per_episode_corr = [stats.pearsonr(e, c)[0] for e, c in zip(env.env_rewards, env.clip_rewards)]\n",
    "    # print(per_episode_corr)\n",
    "    # if nan return 0 correlation\n",
    "    per_episode_corr = [0 if math.isnan(corr) else corr for corr in per_episode_corr]\n",
    "    \n",
    "    for i in range(env.n_episodes):\n",
    "        env.writer.add_scalar('Per episode correlation', per_episode_corr[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "746fe9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('RN50x64', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion2b_e16'),\n",
       " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14', 'laion400m_e31'),\n",
       " ('ViT-L-14', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
       " ('ViT-L-14-336', 'openai'),\n",
       " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
       " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
       " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
       " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
       " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fcd71de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v1'\n",
    "DQN_POLICY = 'MlpPolicy' \n",
    "\n",
    "MODEL = 'RN50'#'ViT-B-32-quickgelu'\n",
    "PRETRAINED = 'yfcc15m'#'laion400m_e32'\n",
    "\n",
    "COMMENT = 'cuda_test'\n",
    "\n",
    "PROMPT = 'White background, brown vertical pole in the middle, on top of the black box, vertically aligned'\n",
    "N_STEPS = 2000\n",
    "\n",
    "EXP_NAME = f'{ENV_NAME}_{PROMPT}_{MODEL}_{N_STEPS}_{COMMENT}'\n",
    "\n",
    "EXP_PATH = f'experiments/{EXP_NAME}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c17a577c-3f55-40cc-9730-9e80b721dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms(MODEL, pretrained=PRETRAINED)\n",
    "tokenizer = open_clip.get_tokenizer(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c21621cb-7d5d-4ed0-8ceb-d2d1b2415ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea2ccfdc97c4872bd765c32b7c4bfd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">C:\\Users\\vaici\\anaconda3\\envs\\ani\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:406: UserWarning: [WinError \n",
       "-2147417850] Cannot change thread mode after it is set\n",
       "  warnings.warn(str(err))\n",
       "</pre>\n"
      ],
      "text/plain": [
       "C:\\Users\\vaici\\anaconda3\\envs\\ani\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:406: UserWarning: [WinError \n",
       "-2147417850] Cannot change thread mode after it is set\n",
       "  warnings.warn(str(err))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m agent \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, cl_env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEXP_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mppo/\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# model.learn(total_timesteps=25_000)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mrun_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcl_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopen_clip_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mENV_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEXP_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCOMMENT\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [8], line 5\u001b[0m, in \u001b[0;36mrun_exp\u001b[1;34m(agent, env, prompt, clip_model_name, env_name, exp_path, n_steps, notes)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(exp_path):\n\u001b[0;32m      3\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(exp_path)\n\u001b[1;32m----> 5\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m agent\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/agent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m corr \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mpearsonr([\u001b[38;5;28msum\u001b[39m(e) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m env\u001b[38;5;241m.\u001b[39menv_rewards], [\u001b[38;5;28msum\u001b[39m(e) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m env\u001b[38;5;241m.\u001b[39mclip_rewards])[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ani\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:317\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m: PPOSelf,\n\u001b[0;32m    305\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PPOSelf:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ani\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:262\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 262\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ani\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:181\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, gym\u001b[38;5;241m.\u001b[39mspaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[0;32m    179\u001b[0m     clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 181\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ani\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ani\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 43\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ani\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn [6], line 34\u001b[0m, in \u001b[0;36mCLIPEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     32\u001b[0m next_st, rwd, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     33\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m clip_rwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_clip_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_rewards_per_episode) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_rewards_per_episode\u001b[38;5;241m.\u001b[39mappend(clip_rwd)\n",
      "Cell \u001b[1;32mIn [6], line 68\u001b[0m, in \u001b[0;36mCLIPEnv.get_clip_reward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     66\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(Image\u001b[38;5;241m.\u001b[39mfromarray(np\u001b[38;5;241m.\u001b[39muint8(state)))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[1;32m---> 68\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     image_features \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ani\\lib\\site-packages\\open_clip\\model.py:182\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[1;34m(self, image, normalize)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 182\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mnormalize(features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m normalize \u001b[38;5;28;01melse\u001b[39;00m features\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ani\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ani\\lib\\site-packages\\open_clip\\modified_resnet.py:174\u001b[0m, in \u001b[0;36mModifiedResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 174\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m    176\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ani\\lib\\site-packages\\open_clip\\modified_resnet.py:169\u001b[0m, in \u001b[0;36mModifiedResNet.stem\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    167\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[0;32m    168\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m--> 169\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact3(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn3\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    170\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ani\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ani\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:151\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats:\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_batches_tracked\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# use cumulative moving average\u001b[39;00m\n\u001b[0;32m    153\u001b[0m             exponential_average_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "writer = SummaryWriter(EXP_PATH)\n",
    "\n",
    "cl_env = CLIPEnv(env, model, preprocess, tokenizer, PROMPT, writer)\n",
    "\n",
    "# agent = DQN(DQN_POLICY, \n",
    "#             cl_env, \n",
    "#             verbose=0, \n",
    "#             learning_starts=1000, \n",
    "#             buffer_size=15000, \n",
    "#             target_update_interval=500,\n",
    "#             tensorboard_log=f'{EXP_PATH}dqn/',\n",
    "#             device='cuda',\n",
    "#             exploration_fraction=0.5,\n",
    "#             exploration_initial_eps=0.5,\n",
    "#             exploration_final_eps=0.1)\n",
    "\n",
    "\n",
    "agent = PPO(\"MlpPolicy\", cl_env, verbose=0, tensorboard_log=f'{EXP_PATH}ppo/', device='cuda')\n",
    "# model.learn(total_timesteps=25_000)\n",
    "\n",
    "\n",
    "run_exp(agent, cl_env, PROMPT, f'open_clip_{MODEL}', ENV_NAME, EXP_PATH, N_STEPS,\n",
    "        COMMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8dd6a4",
   "metadata": {},
   "source": [
    "### Evaluate prompts for good/bad situations in CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d94f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_example = Image.open(\"cartpole_examples/good.png\") \n",
    "bad_example = Image.open(\"cartpole_examples/bad.png\") \n",
    "\n",
    "def evaluate_prompt_cartpole(prompt):\n",
    "    env = gym.make(ENV_NAME)\n",
    "    writer = SummaryWriter(EXP_PATH)\n",
    "    cl_env = CLIPEnv(env, model, preprocess, tokenizer, prompt, writer)\n",
    "\n",
    "    good_reward = cl_env.get_clip_reward(good_example)\n",
    "    bad_reward = cl_env.get_clip_reward(bad_example)\n",
    "    \n",
    "    print(f'Good state clip reward: {good_reward}')\n",
    "    print(f'Bad state clip reward: {bad_reward}')\n",
    "    print(f'Diff: {good_reward - bad_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65ec8e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good state clip reward: 0.14994792640209198\n",
      "Bad state clip reward: 0.10380738228559494\n",
      "Diff: 0.04614054411649704\n"
     ]
    }
   ],
   "source": [
    "evaluate_prompt_cartpole('White background, brown vertical pole in the middle, on top of the black box, vertically aligned') # 'RN50' 'yfcc15m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e02c36d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good state clip reward: 0.12130261957645416\n",
      "Bad state clip reward: 0.09950494766235352\n",
      "Diff: 0.021797671914100647\n"
     ]
    }
   ],
   "source": [
    "evaluate_prompt_cartpole('White background, brown vertical pole in the middle, on top of the black box, perpendicular to each other')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de59dde-4c54-4fed-8a9b-bae872c2963d",
   "metadata": {},
   "source": [
    "# Evaluate the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ecbc0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_prompt(prompt, agent, env, rewards_storage, n_steps=100):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    for _ in range(n_steps):\n",
    "        if done:\n",
    "            break\n",
    "        action, _states = agent.predict(obs)\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "    rewards_storage[prompt] = (env.env_rewards.copy(), env.clip_rewards.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cdc05740-9474-425e-94ce-819df6f2b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rewards = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "cl_env = CLIPEnv(env, model, preprocess, tokenizer, PROMPT, writer)\n",
    "\n",
    "# agent = DQN.load(f\"{EXP_PATH}/agent\", env=cl_env)\n",
    "agent = PPO.load(f\"{EXP_PATH}/agent\", env=cl_env)\n",
    "experiment_prompt(PROMPT, agent, cl_env, prompt_rewards, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178115d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prompt_rewards[PROMPT][0][0])\n",
    "plt.title('Env rewards (1 episode)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fbeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prompt_rewards[PROMPT][1][0])\n",
    "plt.title('CLIP rewards (1 episode)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4ef97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ani_env",
   "language": "python",
   "name": "ani_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "af2b1c113fb30453ab0f38cb28889d45a8255f9a09eebc90dd674ef9d0f3b315"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
