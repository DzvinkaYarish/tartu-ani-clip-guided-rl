{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3fa1a94-14ff-4b0a-a918-d4e5f69b28c5",
   "metadata": {},
   "source": [
    "```bash\n",
    "conda create --name ani python=3.8\n",
    "conda install -c conda-forge jupyterlab\n",
    "pip install open_clip_torch\n",
    "pip install stable-baselines3[extra]\n",
    "pip install gym[all]\n",
    "pip install pyglet==1.5.27\n",
    "pip install tensorboardX\n",
    "conda install -c anaconda ipywidgets\n",
    "conda install -c anaconda scipy \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be7f60-1cd4-4afb-8829-342980047596",
   "metadata": {},
   "source": [
    "## Experiments\n",
    " - 2 environments: CartPole and LunarLander\n",
    " - clip models: openclip https://github.com/mlfoundations/open_clip(different models), cloob https://github.com/ml-jku/cloob\n",
    " - different prompts\n",
    "     - try several prompts at the same time which describe different states and then the rwd is formulated based on to which prompt the env image is most similar\n",
    "     - ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee8f09a-40c5-4e52-bb46-75dc38d96a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import DQN\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ec5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_view_window():\n",
    "        from gym.envs.classic_control import rendering\n",
    "        org_constructor = rendering.Viewer.__init__\n",
    "\n",
    "        def constructor(self, *args, **kwargs):\n",
    "            org_constructor(self, *args, **kwargs)\n",
    "            self.window.set_visible(visible=False)\n",
    "\n",
    "        rendering.Viewer.__init__ = constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741daef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_view_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03579506-1dc0-4fbe-b6d8-b9bc6ad250fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEnv():\n",
    "    def __init__(self, env, clip_model, clip_preprocess, tokenizer, prompt, writer):\n",
    "        self.env = env\n",
    "        self.model = clip_model\n",
    "        self.preprocess = clip_preprocess\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_features = self.model.encode_text(self.tokenizer([prompt]))\n",
    "        self.text_features /= self.text_features.norm(dim=-1, keepdim=True)\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.metadata = self.env.metadata\n",
    "        self.clip_rewards_per_episode = []\n",
    "        self.env_rewards_per_episode = []\n",
    "        \n",
    "        self.clip_rewards = []\n",
    "        self.env_rewards = []\n",
    "        \n",
    "        self.writer = writer\n",
    "        self.n_steps = 0\n",
    "        self.n_episodes = 0\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def close(self):\n",
    "        return self.env.close()\n",
    "    \n",
    "    def step(self, action):\n",
    "        next_st, rwd, done, info = self.env.step(action)\n",
    "        img = self.env.render(mode=\"rgb_array\")\n",
    "        clip_rwd = self.get_clip_reward(img)\n",
    "        \n",
    "        if len(self.clip_rewards_per_episode) == 0:\n",
    "            self.clip_rewards_per_episode.append(clip_rwd)\n",
    "            self.env_rewards_per_episode.append(rwd)\n",
    "        else:\n",
    "            self.clip_rewards_per_episode.append(self.clip_rewards_per_episode[-1] + clip_rwd)\n",
    "            self.env_rewards_per_episode.append(self.env_rewards_per_episode[-1] + rwd)\n",
    "        \n",
    "     \n",
    "        if done:\n",
    "            #self.writer.add_scalar('episode_length',  len(self.env_rewards_per_episode), self.n_episodes)\n",
    "            \n",
    "            self.writer.add_scalar('episode_rewards/env_reward',  sum(self.env_rewards_per_episode), self.n_episodes)\n",
    "            self.writer.add_scalar('episode_rewards/clip_reward', sum(self.clip_rewards_per_episode) , self.n_episodes)\n",
    "            \n",
    "            self.env_rewards.append(self.env_rewards_per_episode)\n",
    "            self.clip_rewards.append(self.clip_rewards_per_episode)\n",
    "\n",
    "            self.env_rewards_per_episode = []\n",
    "            self.clip_rewards_per_episode = []\n",
    "            \n",
    "            self.n_episodes += 1\n",
    "            \n",
    "        \n",
    "        self.n_steps += 1\n",
    "\n",
    "        return next_st, rwd, done, info\n",
    "    \n",
    "    def get_clip_reward(self, state):\n",
    "        image = self.preprocess(Image.fromarray(np.uint8(state))).unsqueeze(0)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            image_features = self.model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            sim = (image_features @ self.text_features.T)\n",
    "        return sim.cpu().detach().numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c9f0393-6e67-4f26-a621-603a1a731dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file experiments already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb0acfd6-3d91-48cf-856a-ce30ea2eb63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(agent, env, prompt, clip_model_name, env_name, exp_path, n_steps, notes=''):\n",
    "    if not os.path.exists(exp_path):\n",
    "        os.mkdir(exp_path)\n",
    "    \n",
    "    agent.learn(total_timesteps=n_steps, progress_bar=True)\n",
    "    agent.save(f\"{exp_path}/agent\")\n",
    "    \n",
    "    corr = stats.pearsonr([sum(e) for e in env.env_rewards], [sum(e) for e in env.clip_rewards])[0]\n",
    "    m_rwd = np.mean([sum(e) for e in env.env_rewards[-10:]])\n",
    "    results = {\n",
    "        'env_name': env_name,\n",
    "        'prompt': prompt,\n",
    "        'clip_model_name': clip_model_name,\n",
    "        'correlation': corr,\n",
    "        'mean_env_rwd_over_last_10_episodes': m_rwd,\n",
    "        'n_episodes': env.n_episodes,\n",
    "        'n_steps': env.n_steps,\n",
    "         'notes': notes,\n",
    "    }\n",
    "    with open(f'{exp_path}/results.json', 'w') as f:\n",
    "        json.dump(results, f)\n",
    "    \n",
    "    # compute correlation between env and clip rewards for each episode separately\n",
    "    per_episode_corr = [stats.pearsonr(e, c)[0] for e, c in zip(env.env_rewards, env.clip_rewards)]\n",
    "    # if nan return 0 correlation\n",
    "    per_episode_corr = [0 if math.isnan(corr) else corr for corr in per_episode_corr]\n",
    "    \n",
    "    for i in range(env.n_episodes):\n",
    "        env.writer.add_scalar('Per episode correlation', per_episode_corr[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "746fe9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('RN50x64', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion2b_e16'),\n",
       " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14', 'laion400m_e31'),\n",
       " ('ViT-L-14', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
       " ('ViT-L-14-336', 'openai'),\n",
       " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
       " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
       " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
       " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
       " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fcd71de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v1'\n",
    "DQN_POLICY = 'MlpPolicy' #'LnCnnPolicy'\n",
    "\n",
    "MODEL = 'RN50'#'ViT-B-32-quickgelu'\n",
    "PRETRAINED = 'yfcc15m'#'laion400m_e32'\n",
    "\n",
    "COMMENT = 'Correlation_test_1'\n",
    "\n",
    "PROMPT = 'White background, brown vertical pole in the middle, on top of the black box, vertically aligned'\n",
    "N_STEPS = 2000\n",
    "\n",
    "EXP_NAME = f'{ENV_NAME}_{PROMPT}_{MODEL}_{PRETRAINED}_{N_STEPS}_{COMMENT}'\n",
    "\n",
    "EXP_PATH = f'experiments/{EXP_NAME}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c17a577c-3f55-40cc-9730-9e80b721dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms(MODEL, pretrained=PRETRAINED)\n",
    "tokenizer = open_clip.get_tokenizer(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c21621cb-7d5d-4ed0-8ceb-d2d1b2415ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96fd1c7278941ae9c061f62f40891d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">C:\\Users\\vaici\\anaconda3\\envs\\ani\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:406: UserWarning: [WinError \n",
       "-2147417850] Cannot change thread mode after it is set\n",
       "  warnings.warn(str(err))\n",
       "</pre>\n"
      ],
      "text/plain": [
       "C:\\Users\\vaici\\anaconda3\\envs\\ani\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:406: UserWarning: [WinError \n",
       "-2147417850] Cannot change thread mode after it is set\n",
       "  warnings.warn(str(err))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaici\\anaconda3\\envs\\ani\\lib\\site-packages\\scipy\\stats\\stats.py:4023: PearsonRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "writer = SummaryWriter(EXP_PATH)\n",
    "\n",
    "cl_env = CLIPEnv(env, model, preprocess, tokenizer, PROMPT, writer)\n",
    "\n",
    "agent = DQN(DQN_POLICY, \n",
    "            cl_env, \n",
    "            verbose=0, \n",
    "            learning_starts=1000, \n",
    "            buffer_size=15000, \n",
    "            target_update_interval=500,\n",
    "            tensorboard_log=f'{EXP_PATH}dqn/',\n",
    "            exploration_fraction=0.5,\n",
    "            exploration_initial_eps=0.5,\n",
    "            exploration_final_eps=0.2)\n",
    "\n",
    "\n",
    "run_exp(agent, cl_env, PROMPT, f'open_clip_{MODEL}', ENV_NAME, EXP_PATH, N_STEPS,\n",
    "        'all additional info about experiment goes here')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8dd6a4",
   "metadata": {},
   "source": [
    "### Evaluate prompts for good/bad situations in CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d94f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_example = Image.open(\"cartpole_examples/good.png\") \n",
    "bad_example = Image.open(\"cartpole_examples/bad.png\") \n",
    "\n",
    "def evaluate_prompt_cartpole(prompt):\n",
    "    env = gym.make(ENV_NAME)\n",
    "    writer = SummaryWriter(EXP_PATH)\n",
    "    cl_env = CLIPEnv(env, model, preprocess, tokenizer, prompt, writer)\n",
    "\n",
    "    good_reward = cl_env.get_clip_reward(good_example)\n",
    "    bad_reward = cl_env.get_clip_reward(bad_example)\n",
    "    \n",
    "    print(f'Good state clip reward: {good_reward}')\n",
    "    print(f'Bad state clip reward: {bad_reward}')\n",
    "    print(f'Diff: {good_reward - bad_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65ec8e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good state clip reward: 0.14994792640209198\n",
      "Bad state clip reward: 0.10380738228559494\n",
      "Diff: 0.04614054411649704\n"
     ]
    }
   ],
   "source": [
    "evaluate_prompt_cartpole('White background, brown vertical pole in the middle, on top of the black box, vertically aligned') # 'RN50' 'yfcc15m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e02c36d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good state clip reward: 0.12130261957645416\n",
      "Bad state clip reward: 0.09950494766235352\n",
      "Diff: 0.021797671914100647\n"
     ]
    }
   ],
   "source": [
    "evaluate_prompt_cartpole('White background, brown vertical pole in the middle, on top of the black box, perpendicular to each other')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de59dde-4c54-4fed-8a9b-bae872c2963d",
   "metadata": {},
   "source": [
    "# Evaluate the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_prompt(prompt, agent, env, rewards_storage, n_steps=100):\n",
    "    obs = env.reset()\n",
    "    for _ in range(n_steps):\n",
    "        action, _states = agent.predict(obs)\n",
    "        obs, rewards, dones, info = env.step(action)\n",
    "    rewards_storage[prompt] = (env.env_rewards.copy(), env.clip_rewards.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc05740-9474-425e-94ce-819df6f2b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rewards = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "cl_env = CLIPEnv(env, model, preprocess, tokenizer, prompt, writer)\n",
    "\n",
    "agent = DQN.load(f\"{EXP_PATH}/agent\", env=cl_env)\n",
    "experiment_prompt(prompt, agent, cl_env, prompt_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1433b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178115d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prompt_rewards[prompt][0][0])\n",
    "plt.title('Env rewards (1 episode)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fbeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(prompt_rewards[prompt][1][0])\n",
    "plt.title('CLIP rewards (1 episode)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4ef97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ani_env",
   "language": "python",
   "name": "ani_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "af2b1c113fb30453ab0f38cb28889d45a8255f9a09eebc90dd674ef9d0f3b315"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
